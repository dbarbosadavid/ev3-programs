# ü§ñ Seguidor de Linha (Q-Learning) com EV3

## üìÑ Vis√£o Geral do Projeto

Este projeto demonstra a aplica√ß√£o de **Aprendizado por Refor√ßo (Reinforcement Learning - RL)**, especificamente o algoritmo **Q-Learning**, para treinar um rob√¥ **LEGO Mindstorms EV3** a seguir uma linha.

Diferente dos m√©todos de controle convencionais (como PID), o rob√¥ aprende as melhores a√ß√µes (virar √† esquerda, direita, ou seguir em frente) atrav√©s de **tentativa e erro**, otimizando sua performance com base em recompensas.

---

## üß† Conceito Principal: Q-Learning

O Q-Learning √© o cora√ß√£o deste c√≥digo. Ele usa a matriz **Q** (implementada como um dicion√°rio em Python) para armazenar o "valor de qualidade" de cada a√ß√£o em um determinado estado. 

| Termo | Vari√°vel no C√≥digo | Descri√ß√£o |
| :--- | :--- | :--- |
| **Q-Table** | `Q` | Dicion√°rio que mapeia **estados** para **a√ß√µes** e seus respectivos valores de qualidade. |
| **Estados** | `get_state()` | As quatro condi√ß√µes do ambiente: `both_black`, `left_black`, `right_black`, e `lost`. |
| **A√ß√µes** | `actions` | Movimentos que o rob√¥ pode realizar: `"forward"`, `"left"`, `"right"`. |
| **Recompensa** | `reward(state)` | Fun√ß√£o que atribui um valor (positivo ou negativo) com base no novo estado atingido ap√≥s uma a√ß√£o. |
| **Explora√ß√£o** | `epsilon` | Define a chance de o rob√¥ escolher uma a√ß√£o aleat√≥ria (explora√ß√£o). |
| **F√≥rmula de Atualiza√ß√£o** | A equa√ß√£o que ajusta os valores de `Q` com base na recompensa (`r`), taxa de aprendizado (`alpha`) e fator de desconto (`gamma`). |

---

## ‚öôÔ∏è Configura√ß√£o e Hiperpar√¢metros

Os hiperpar√¢metros determinam a velocidade e a qualidade do aprendizado do rob√¥.

| Par√¢metro | Vari√°vel no C√≥digo | Valor Padr√£o | Fun√ß√£o |
| :--- | :--- | :--- | :--- |
| **Taxa de Aprendizado** | `alpha` | `0.5` | Qu√£o rapidamente a nova informa√ß√£o se sobrep√µe √† informa√ß√£o antiga. |
| **Fator de Desconto** | `gamma` | `0.8` | Import√¢ncia das recompensas futuras (longo prazo). |
| **Taxa de Explora√ß√£o** | `epsilon` | `1.0` | Probabilidade inicial de o rob√¥ *explorar* (escolhe uma a√ß√£o aleat√≥ria). **Decai ao longo do tempo.** |
| **Velocidade Base** | `velocidade` | `100` | Velocidade base dos motores (em graus por segundo). |

---

## üí° Funcionamento do Algoritmo

O rob√¥ opera em um loop cont√≠nuo de aprendizado e execu√ß√£o:

1.  **Observa o Estado (`get_state`):** O rob√¥ l√™ os sensores de cor e determina em qual dos 4 estados discretos ele se encontra.
2.  **Escolhe a A√ß√£o (`choose_action`):** Utiliza a pol√≠tica **epsilon-greedy**:
    * No in√≠cio, o alto `epsilon` for√ßa a **explora√ß√£o** (a√ß√µes aleat√≥rias).
    * Com o decaimento do `epsilon`, o rob√¥ passa a priorizar a **explota√ß√£o** (a a√ß√£o com o maior Q-valor conhecido).
3.  **Executa e Recompensa:** O rob√¥ executa a a√ß√£o, observa o novo estado e recebe a recompensa correspondente:
    * **Positivo (+20):** Se estiver bem centralizado (`both_black`).
    * **Negativo (-20):** Se estiver perdido (`lost`).
4.  **Atualiza a Tabela Q:** A f√≥rmula de Q-Learning ajusta o Q-valor da a√ß√£o que acabou de ser executada, consolidando o aprendizado.

---

## üõ†Ô∏è Componentes de Hardware

O rob√¥ requer a seguinte configura√ß√£o de portas:

| Componente | Vari√°vel no C√≥digo | Porta |
| :--- | :--- | :--- |
| **EV3 Brick** | `ev3` | - |
| **Motor Esquerdo** | `motor_esquerda` | `Port.A` |
| **Motor Direito** | `motor_direita` | `Port.B` |
| **Sensor de Cor** | `color_esquerda` | `Port.S1` |
| **Sensor de Cor** | `color_direita` | `Port.S2` |
